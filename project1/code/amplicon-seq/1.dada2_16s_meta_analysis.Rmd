---
title: "dada2_pipeline"
author: "Ricardo Silva"
date: "10/10/2022"
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval=FALSE, 
                      message=FALSE, 
                      warning=FALSE)
```

```{r CLEAR EVERYTHING, eval=FALSE, include=FALSE}
# unload all non-base packages
lapply(names(sessionInfo()$otherPkgs), function(pkgs)
  detach(
    paste0('package:', pkgs),
    character.only = T,
    unload = T,
    force = T
))

rm(list=ls())
```

# Packages and initial steps

Make sure pacman and Bioconductor are installed

```{r}
# install pacman package, after load the packages
if (!require("pacman", "BiocManager", quietly = TRUE))
    install.packages("pacman","BiocManager")

library(pacman)

# this funciton will load the package or install it if necessary
# This will install packages available on CRAN, or, 
# if the user has previously installed Bioconductor, from the Bioconductor repos.
# 
p_load(tidyverse, patchwork, here, Biostrings, ShortRead, dada2, doParallel, update = F)

```

```{r load libraries, message=FALSE, warning=FALSE, include=FALSE}
# suppressPackageStartupMessages({
#   library(dada2)
#   library(Biostrings) # for primer identification
#   library(tidyverse)
#   library(ShortRead) # for primer identification
#   library(patchwork)
# })
packageVersion("dada2")
```

# create directories

```{r}
# dir.create("scripts")
# dir.create("outputs")
```

```{r}
#setwd(folder_path)
# source(paste0(folder_path,"/scripts/theme_publication.R"))
files_path <- here("folder XXXXX")
# checking
head(list.files(files_path))


```

We need to know how fastaq files were generated.
For instance, fastq files can be generated by 2x300bp Illumina Miseq amplicon.
This menas that each read (forward and reverse) has ~300bp length.


DADA2
expects there to be fastq file(s) for each samples (in our case one
forward and one reverse, for each sample). If the samples are
demultiplexed without non-biological nucleotides (e.g. primers,
adapters, linkers, etc.), dada2 pipeline proceeds as follows:

Filter and trim: `filterAndTrim()` Dereplicate: `derepFastq()` Learn
error rates: `learnErrors()` Infer sample composition: `dada()` Merge
paired reads: `mergePairs()` Make sequence table: `makeSequenceTable()`
Remove chimeras: `removeBimeraDenovo()`

<https://benjjneb.github.io/dada2/tutorial.html>
<https://astrobiomike.github.io/amplicon/dada2_workflow_ex>
<https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html>

# Preparation

```{r names}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(files_path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(files_path, pattern="_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- fnFs %>% basename() %>% str_split("_") %>% map_chr(., 1) 
# sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#increasing R memory
# options(future.globals.maxSize = 20000 * 1024^2)
```

# Identify primers

To check the primers in the sequences please go to the
`map_count_primers.Rmd` file

# Remove Primers

The FWD and REV primers can be found in some forward and reverse reads in
its reverse orientation, respectively. However, if there is a 
small number (\~200) of reads that still have some primers on them, 
they won't interfere with subsequent analysis.
<https://github.com/benjjneb/dada2/issues/675>

GO TO `trimming_primers.sh`

Using cutadapt:

in flag -a we need to put the primers for the forward read, giving it the forward primer (in normal orientation), followed by three dots (required by cutadapt to know they are “linked”, with bases in between them, rather than right next to each other), then the reverse complement of the reverse primer

use this [site](https://arep.med.harvard.edu/labgc/adnan/projects/Utilities/revcomp.html) for converting to reverse complement while treating degenerate bases properly.

Then for the reverse reads, specified with the -A flag, we give it the reverse primer (in normal 5’-3’ orientation), three dots, and then the reverse complement of the forward primer. Both of those have a ^ symbol in front at the 5’ end indicating they should be found at the start of the reads (which is the case with this particular setup).

More info about [cutadapt](https://cutadapt.readthedocs.io/en/stable/recipes.html#trimming-amplicon-primers-from-paired-end-reads)

# Quality trimming/filtering

Refs:

-   <https://forum.qiime2.org/t/where-to-trim-reads/5741/22>
-   <https://forum.qiime2.org/t/quality-score-and-trimming-in-dada2/541/2>
-   <https://github.com/benjjneb/dada2/issues/195>
-   <https://github.com/benjjneb/dada2/issues/236>
-   <https://forum.qiime2.org/t/questions-about-v3-v4-primers-for-16s-rrna-amplicon-sequencing-and-calculating-overlap/20250/2>

First, inspect quality profles

Visualize the quality profiles of the Forward and reverse reads.

In gray-scale is a heat map of the frequency of each quality score at
each base position. The mean quality score at each position is shown by
the green line, and the quartiles of the quality score distribution by
the orange lines. The red line shows the scaled proportion of reads that
extend to at least that position.

```{r quality profiles}
plotQualityProfile(fnFs[10:12])

# make a string to subset
n <- 1:length(fnFs)
#samples.str <- paste0("-",n,"_") 

# create folder
# dir.create("outputs/quality_control_F/")
# dir.create("outputs/quality_control_R")

# map function F seqs
quality.plotsF <- split(n, ceiling(seq_along(n)/6)) %>% 
  map(. %>% 
        map(., ~str_subset(fnFs, sample.names[.])) %>% plotQualityProfile()
      ) %>% 
  map(~wrap_plots(.))
# save plots
map2(paste0(folder_path,"/outputs/quality_control_F/group_sample_",names(quality.plotsF),".tiff"),quality.plotsF, ggsave)

# map function R seqs
quality.plotsR <- split(n, ceiling(seq_along(n)/6)) %>% 
  map(. %>% 
        map(., ~str_subset(fnRs, sample.names[.])) %>% plotQualityProfile()
      ) %>% 
  map(~wrap_plots(.))
# save plots
map2(paste0(folder_path,"/outputs/quality_control_R/group_sample_",names(quality.plotsR),".tiff"), quality.plotsR, ggsave)

```

formula for calculating overlap

`(length of forward read) + (length of reverse read) − (length of amplicon) − (length of forward read − --p-trunc-len-f value) − (length of reverse read − --p-trunc-len-r value) = overlap`.

So, for example, if we picked `--p-trunc-len-f = 280` and
`--p-trunc-len-r = 250`, we'd have `300 + 300 − 464 − 20 − 50 = 66 bp`
overlap

For instace, our paired-end reads cover the V1-V3 regions using the 27F (Lane 1991)
and 519R (Lane et al. 1993) primers. It should yield an amplicon with
492 bp long (519 - 27 bps). In calculating the length of overlapping
bases we get 108 bp (600 - 492) overlap (600 = 2x300bp Illumina Miseq).
If we truncate both forward and reverse at 290 and 255, respectively,
the overlap would be 53 bp (492 - 290 - 255).

NOTE!! For the F/R reads to be successfully merged, `trunc-len-f` +
`trunc-len-r` must be greater than the length of the amplicon + 20
nucleotides (the 20 nts is the length of the overlap).
<https://github.com/qiime2/q2-dada2/issues/52>

```{r filtering}

# Assign the filenames for the filtered fastq.gz files.
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(files_path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(files_path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# run the function on good sequences
filtered_out <- filterAndTrim(fnFs, # “forward_reads” - input
                                   filtFs,  # “forward_reads” - output filtered
                                   fnRs, # “reverse_reads” - input
                                   filtRs,  # “forward_reads” - output filtered
                                   maxEE=c(2,4), # quality filtering threshold
                                   rm.phix=TRUE, # removes any reads that match the PhiX bacteriophage genome
                                #   minLen=30, # minimum length reads we want to keep after trimming
                                   truncLen=c(290,255), # minimum size to trim the forward and reverse reads (keep the QS above 30 overall) - Shorter sequences are discarded
                                   compress = TRUE, # gzipped fastq files 
                                   truncQ = 2, #  trims all bases after the first quality score of 2
                                   multithread=TRUE, # TRUE if working in a HPC, otherwise FALSE (e.g., Windows machine)
                                   verbose = T) 

head(filtered_out)
df_reads <- filtered_out %>% as_tibble() %>% mutate(perc = (reads.out/reads.in)*100)

# create new folder
path.dada2 <- file.path(files_path, "dada2")
if(!dir.exists(path.dada2)) dir.create(path.dada2)
rm(path.dada2)

# save
saveRDS(filtered_out, paste0(files_path, "/dada2/filtered_trim.RDS"))


```

```{r checking}
plotQualityProfile(filtFs[1:3])
plotQualityProfile(filtRs[1:3])
```

# Learn the Error Rates

We will generate a parametric error model of our data by learning the
specific error-signature of our dataset.

We can use `multithread=TRUE` on Windows (and probably should if
computation time is an issue) for every command except `filterAndTrim`.
<https://github.com/benjjneb/dada2/issues/1100>

```{r error rate}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

# save RDS
saveRDS(errF, paste0(files_path, "/dada2/errF.RDS"))
saveRDS(errF, paste0(files_path, "/dada2/errR.RDS"))
```

The plots below show the error rates for each possible transition (A→C,
A→G, ...) are shown. The black line shows the estimated error rates
after convergence of the machine-learning algorithm. The red line shows
the error rates expected under the nominal definition of the Q-score. We
want the observed (black dots) to track well with the estimated (black
line). Here the estimated error rates (black line) are a good fit to the
observed rates (points), and the error rates drop with increased quality
as expected. Everything looks reasonable and we proceed with confidence.

```{r checking sanity}
# sanity check
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

# Dereplication

The next thing we want to do is "dereplicate" the filtered fastq files.
During dereplication, we condense the data by collapsing together all
reads that encode the same sequence, which significantly reduces later
computation times. Instead of keeping 100 identical sequences and doing
all downstream processing to all 100, you can keep/process one of them,
and just attach the number 100 to it.

```{r dereplication}
derep_forward <- derepFastq(filtFs, verbose=TRUE)
names(derep_forward) <- sample.names
derep_reverse <- derepFastq(filtRs, verbose=TRUE)
names(derep_reverse) <- sample.names

# save RDS
saveRDS(derep_forward, paste0(files_path, "/dada2/derep_forward.RDS"))
saveRDS(derep_reverse, paste0(files_path, "/dada2/derep_reverse.RDS"))

#derep_forward <- readRDS(paste0(files_path, "/dada2/derep_forward.RDS"))
#derep_reverse <- readRDS(paste0(files_path, "/dada2/derep_reverse.RDS"))
```

# Inferring ASVs/ Sample composition inference

Here's where DADA2 gets to do what it was born to do, that is to do its
best to infer true biological sequences. Pooling information across
samples can increase sensitivity to sequence variants that may be
present at very low frequencies in multiple samples.
`dada(..., pool="pseudo")` performs pseudo-pooling, in which samples are
processed independently after sharing information between samples,
approximating pooled sample inference in linear time. In many cases,
especially when samples are repeatedly drawn from the same source such
as in longitudinal experiments, pseudo-pooling can provide a more
accurate description of ASVs at very low frequencies (e.g. present in
1-5 reads per sample).

```{r ASV inference}
dada_forward <- dada(derep_forward, err=errF, 
                   #  pool="pseudo", 
                     multithread=TRUE)
# save RDS
saveRDS(dada_forward, paste0(files_path, "/dada2/dada_forward.RDS"))
#dada_forward <- read_rds(paste0(files_path, "/dada2/dada_forward.RDS"))

dada_reverse <- dada(derep_reverse, err=errR, 
                     #pool="pseudo", 
                     multithread=TRUE)
# save RDS
saveRDS(dada_reverse, paste0(files_path, "/dada2/dada_reverse.RDS"))
#dada_reverse <- read_rds(paste0(files_path, "/dada2/dada_reverse.RDS"))
```

Inspecting the returned dada-class object:

```{r insp dada obj}
dada_forward[[6]]
dada_reverse[[6]]
```

# Merging paired forward and reverse reads

Merging is performed by aligning the denoised forward reads with the
reverse-complement of the corresponding denoised reverse reads, and then
constructing the merged "contig" sequences. By default it requires that
at least 12 bps overlap. We are going to merge using three different
ways and see how they go: 1 - default (DADA2 default) 2 - trimOverhang =
T; and 3 - minimum overlap (`minOverlap`) at 30 with the `trimOverhang`
option to `TRUE` in case any of our reads go passed their opposite
primers

The 1 function returns a `data.frame` corresponding to each successfully
merged unique sequence. The `$forward` and `$reverse` columns record
which forward and reverse sequence contributed to that merged sequence.

```{r merging}
merged_amplicons.default <- mergePairs(dada_forward, derep_forward, 
                               dada_reverse, derep_reverse, 
                             #  trimOverhang = T,
                             #  minOverlap=30, 
                               verbose = T)

# merged_amplicons.overhang.T <- mergePairs(dada_forward, derep_forward,
#                                dada_reverse, derep_reverse,
#                                trimOverhang = T,
#                              #  minOverlap=30,
#                                verbose = T)
# 
# merged_amplicons.min30 <- mergePairs(dada_forward, derep_forward,
#                                dada_reverse, derep_reverse,
#                                trimOverhang = T,
#                                minOverlap=30,
#                                verbose = T)
```

```{r merging}
merged_amplicons <- merged_amplicons.default
# merged_amplicons <- merged_amplicons.overhang.T
# merged_amplicons <- merged_amplicons.min30

head(merged_amplicons[[4]])
# this object holds a lot of information that may be the first place you'd want to look if you want to start poking under the hood
class(merged_amplicons) # list
length(merged_amplicons) # 67 elements in this list, one for each of our samples
names(merged_amplicons) # the names() function gives us the name of each element of the list 

class(merged_amplicons$`D-10B`) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe

names(merged_amplicons$`D-10B`) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"

# remove big objects
rm("derep_forward", 'derep_reverse')
```

# Construct sequence table

We can see from the dimensions of the "seqtab" matrix that we have
15,731 ASVs in both overhang = T and default. Then, we will continue
with the default merged data.

```{r seqtable}
seqtab.overhand.T <- makeSequenceTable(merged_amplicons.overhang.T)
seqtab.default <- makeSequenceTable(merged_amplicons.default)
seqtab.min30 <- makeSequenceTable(merged_amplicons.min30)

# inspect the number of ASVs in each seq table
dim(seqtab.default) # samples X ASVs
dim(seqtab.overhand.T) #
dim(seqtab.min30) # 

# get the chosen one
seqtab <- seqtab.default
class(seqtab) # matrix
merged_amplicons <- merged_amplicons.default

# save RDS
saveRDS(merged_amplicons, paste0(files_path, "/dada2/merged_amplicons.RDS"))
#merged_amplicons <- read_rds(paste0(files_path, "/dada2/merged_amplicons.RDS"))

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

rm('merged_amplicons', 'merged_amplicons.min30', 'merged_amplicons.overhang.T', 'seqtab.overhand.T', 'seqtab.default', 'seqtab.min30')
```

# Chimera identification

The core dada method corrects substitution and indel errors
(insertion--deletion mutations), but chimeras remain. Fortunately, the
accuracy of sequence variants after denoising makes identifying chimeric
ASVs simpler than when dealing with fuzzy OTUs.

```{r chimera}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#   10037 bimeras out of 15731 input sequences
dim(seqtab.nochim) # 3636 ASVs
# dim(seqtab.nochim.min30) #  

# lost 44491 sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab)  # 97%

saveRDS(seqtab.nochim, paste0(files_path, "/dada2/seqtab.nochim.RDS"))
```

Here chimeras make up about 63% of the merged sequence variants, but
when we account for the abundances of those variants we see they account
for less than 4% of the merged sequence reads.

# Overview of counts throughout

As a final check of our progress, we'll look at the number of reads that
made it through each step in the pipeline:

```{r overview}
# set a little function
getN <- function(x) sum(getUniques(x))


# making a little table
summary_tab <- data.frame(row.names = sample.names, 
                          dada2_input = filtered_out[,1],
                          filtered = filtered_out[,2], 
                          input_percent = round((filtered_out[,2]/filtered_out[,1])*100,2),
                          dada_f = sapply(dada_forward, getN),
                          dada_r = sapply(dada_reverse, getN), 
                          merged = sapply(merged_amplicons, getN),
                          input_perc = round((sapply(merged_amplicons, getN)/filtered_out[,1])*100,2),
                          filtered_perc = round((sapply(merged_amplicons, getN)/filtered_out[,2])*100, 2),
                          nonchim = rowSums(seqtab.nochim),
                          final_perc_reads_retained = round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 2),
                          merged_perc = round(rowSums(seqtab.nochim)/sapply(merged_amplicons, getN)*100, 2))

summary_tab

# save
summary_tab %>% 
  rownames_to_column("sample_id") %>% 
  write_csv(paste0(getwd(), "/outputs/summary_tab_dada2.csv"))
```

```{r}
# close cluster
# parallel::stopCluster(cl = my_cluster)
```

# Assign taxonomy
